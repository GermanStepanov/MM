---
title: "Упражнение 4"
author: "Степанов Герман ПМИ 3-1"
date: "`r format(Sys.Date(), '%d  %B  %Y')`"
output: html_document
---

```{r setup, include=FALSE}

# загрузка пакетов
library('ISLR')         # загружаем пакет
library('GGally')       # графики совместного разброса переменных
library('lmtest')       # тесты остатков регрессионных моделей
library('FNN')          # алгоритм kNN

knitr::opts_chunk$set(echo = TRUE)

```


Цель: исследовать набор данных Auto {ISLR} с помощью линейной регрессионной модели. Задействовав все возможные регрессоры, сделать вывод о пригодности модели для прогноза. Сравнить с методом k ближайших соседей по MSE на тестовой выборке.


```{r, echo=FALSE}

# константы
my.seed <- 26
train.percent <- 0.85

# загрузка данных
#fileURL <- 'https://sites.google.com/a/kiber-guu.ru/msep/mag-econ/mpg_data.csv?attredirects=0&d=1'

# открываем данные
data(Auto)            
#?Auto 


# преобразуем категориальные переменные в факторы
#Auto <- read.csv(data(Auto), row.names = 1, sep = ';', as.is = T)
#Auto$weight <- as.factor(Auto$weight)

Auto$origin <- as.factor(Auto$origin)

#Auto$horsepower <- as.factor(Auto$horsepower)

Auto <- subset(Auto, select = c(mpg, weight, displacement, horsepower, origin))
Auto 

#head(Auto)

#str(Auto) 

# обучающая выборка
set.seed(my.seed)
inTrain <- sample(seq_along(Auto$mpg), 
                  nrow(Auto) * train.percent)
df.train <- Auto[inTrain, c(colnames(Auto)[-1], colnames(Auto)[1])]
df.test <- Auto[-inTrain, -1]

```


#### Описание переменных

Набор данных Auto содержит переменные:

mpg - миль на галлон

weight - Масса автомобиля (кг.)

displacement - Объем двигателя (куб. дюймов)

horsepower - Мощность двигателя.

origin - Происхождение автомобиля (1. Американский, 2. Европейский, 3. Японский)

Размерность обучающей выборки: n = 397 строк, p = 4 объясняющих переменных. Зависимая переменная – mpg.

## Oписательные статистики по переменным
```{r, echo=FALSE}

summary(df.train)

```


## Cовместный график разброса переменных.

Для более наглядности разобьем график на несколько.
```{r, echo=FALSE, warning=FALSE, error = F}

ggp <- ggpairs(df.train[c(1, 2, 5)], upper = list(combo = 'box'))
print(ggp, progress = F)

ggp <- ggpairs(df.train[3:5], upper = list(combo = 'box'))
print(ggp, progress = F)

```


```{r, echo=FALSE, warning=FALSE, error = F}
# цвета по фактору horsepower
ggpairs(df.train[, c('origin', 'mpg')],
aes(color = origin), upper = list(combo = 'box'))

```


Коробчатые диаграммы на пересечении *mpg* и *origin* показывают, что самому большому выпуску японских  автомобилей  соответствует самый низкий расход миль на галлон. Самый высокий расход миль на галлон соответствует  американски автомобилям .


## Модели

```{r echo = F, warning = F, error = F}

model.1 <- lm(mpg ~ . + origin:weight + origin:displacement + origin:horsepower,
              data = df.train)

summary(model.1)

```
 Совместное влияние *weight:origin2-3* исключаем, т.к. параметры незначимы и недостаточно наблюдений для оценки одного из них. 

```{r echo = F, warning = F, error = F}

model.2 <- lm(mpg ~ .+ origin:displacement + origin:horsepower ,
              data = df.train)
summary(model.2)

```

В модели незначим horsepower, его исключаем.

```{r echo = F, warning = F, error = F}

model.3 <- lm(mpg ~ weight + displacement
             + origin:displacement + origin:horsepower ,
              data = df.train)
summary(model.3)

```

Исключаем origin1:horsepower.


```{r echo = F, warning = F, error = F}

model.4 <- lm(mpg ~ weight + displacement
             + origin:displacement  ,
              data = df.train)
summary(model.4)

```
Совместное влияние displacement:origin2 не значимо, следовательно его исключаем.

```{r echo = F, warning = F, error = F}

model.5 <- lm(mpg ~ weight + displacement
              ,
              data = df.train)
summary(model.5)

```
Коэффицент при факторе displacement незначим, исключаем.


```{r echo = F, warning = F, error = F}

model.5 <- lm(mpg ~ weight 
              ,
              data = df.train)
summary(model.5)

```
Модель значима, с коэфф детерминации 0,7.


# Проверка остатков

```{r echo = F, warning = F, error = F}
# тест Бройша-Пагана
bptest(model.5)

# статистика Дарбина-Уотсона
dwtest(model.5)

# графики остатков
par(mar = c(4.5, 4.5, 2, 1))
par(mfrow = c(1, 3))

# график 1
plot(model.5, 1)

# график 2
plot(model.5, 4)

# график 3
plot(model.5, 5) 

par(mfrow = c(1, 1))

```

Судя по графику слева, остатки не случайны (гомоскедастичны), и их дисперсия непостоянна. В модели есть три влиятельных наблюдения: 326, 330, 323, – которые, однако, не выходят за пределы доверительных границ на третьем графике. Графики остатков заставляют усомниться в том, что остатки удовлетворяют условиям Гаусса-Маркова.


# Сравнение с kNN

```{r echo = F}
# линейная модель
# фактические значения y на тестовой выборке
y.fact <- Auto[-inTrain, ]$mpg
y.model.lm <- predict(model.5, df.test)
MSE.lm <- sum((y.model.lm - y.fact)^2) / length(y.model.lm)


# kNN требует на вход только числовые переменные
df.train.num <- as.data.frame(apply(df.train, 2, as.numeric))
df.test.num <- as.data.frame(apply(df.test, 2, as.numeric))

# цикл по k
for (i in 2:50){
model.knn <- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% 'mpg')], 
                     y = df.train.num[, 'mpg'], 
                     test = df.test.num, k = i)
y.model.knn <-  model.knn$pred
    if (i == 2){
        MSE.knn <- sum((y.model.knn - y.fact)^2) / length(y.model.knn)
    } else {
        MSE.knn <- c(MSE.knn, 
                     sum((y.model.knn - y.fact)^2) / length(y.model.knn))
    }
}
#if (i == 2){# цвета по фактору educ
#ggpairs(df.train[, c('exper', 'educ', 'mpg')],
#aes(color = educ), upper = list(combo = 'box'))

# цвета по фактору forlang
#ggpairs(df.train[, c('exper', 'forlang', 'mpg')],
#aes(color = forlang), upper = list(combo = 'box'))


# график
par(mar = c(4.5, 4.5, 1, 1))
# ошибки kNN
plot(2:50, MSE.knn, type = 'b', col = 'darkgreen',
     xlab = 'значение k', ylab = 'MSE на тестовой выборке')
# ошибка регрессии
lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2)
legend('topright', lty = c(1, 2), pch = c(1, NA), 
       col = c('darkgreen', grey(0.2)), 
       legend = c('k ближайших соседа', 'регрессия (все факторы)'), 
       lwd = rep(2, 2))

```
 $$\frac{\sqrt{MSE_{TEST}}}{\bar{y}_{TEST}} = 0,08 $$



